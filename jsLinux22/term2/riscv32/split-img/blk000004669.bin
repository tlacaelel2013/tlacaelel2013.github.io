// -*- C++ -*-

// Copyright (C) 2007-2019 Free Software Foundation, Inc.
//
// This file is part of the GNU ISO C++ Library.  This library is free
// software; you can redistribute it and/or modify it under the terms
// of the GNU General Public License as published by the Free Software
// Foundation; either version 3, or (at your option) any later
// version.

// This library is distributed in the hope that it will be useful, but
// WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
// General Public License for more details.

// Under Section 7 of GPL version 3, you are granted additional
// permissions described in the GCC Runtime Library Exception, version
// 3.1, as published by the Free Software Foundation.

// You should have received a copy of the GNU General Public License and
// a copy of the GCC Runtime Library Exception along with this program;
// see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
// <http://www.gnu.org/licenses/>.

/** @file parallel/balanced_quicksort.h
 *  @brief Implementation of a dynamically load-balanced parallel quicksort.
 *
 *  It works in-place and needs only logarithmic extra memory.
 *  The algorithm is similar to the one proposed in
 *
 *  P. Tsigas and Y. Zhang.
 *  A simple, fast parallel implementation of quicksort and
 *  its performance evaluation on SUN enterprise 10000.
 *  In 11th Euromicro Conference on Parallel, Distributed and
 *  Network-Based Processing, page 372, 2003.
 *
 *  This file is a GNU parallel extension to the Standard C++ Library.
 */

// Written by Johannes Singler.

#ifndef _GLIBCXX_PARALLEL_BALANCED_QUICKSORT_H
#define _GLIBCXX_PARALLEL_BALANCED_QUICKSORT_H 1

#include <parallel/basic_iterator.h>
#include <bits/stl_algo.h>
#include <bits/stl_function.h>

#include <parallel/settings.h>
#include <parallel/partition.h>
#include <parallel/random_number.h>
#include <parallel/queue.h>

#if _GLIBCXX_PARALLEL_ASSERTIONS
#include <parallel/checkers.h>
#ifdef _GLIBCXX_HAVE_UNISTD_H
#include <unistd.h>
#endif
#endif

namespace __gnu_parallel
{
  /** @brief Information local to one thread in the parallel quicksort run. */
  template<typename _RAIter>
    struct _QSBThreadLocal
    {
      typedef std::iterator_traits<_RAIter> _TraitsType;
      typedef typename _TraitsType::difference_type _DifferenceType;

      /** @brief Continuous part of the sequence, described by an
      iterator pair. */
      typedef std::pair<_RAIter, _RAIter> _Piece;

      /** @brief Initial piece to work on. */
      _Piece _M_initial;

      /** @brief Work-stealing queue. */
      _RestrictedBoundedConcurrentQueue<_Piece> _M_leftover_parts;

      /** @brief Number of threads involved in this algorithm. */
      _ThreadIndex _M_num_threads;

      /** @brief Pointer to a counter of elements left over to sort. */
      volatile _DifferenceType* _M_elements_leftover;

      /** @brief The complete sequence to sort. */
      _Piece _M_global;

      /** @brief Constructor.
       *  @param __queue_size size of the work-stealing queue. */
      _QSBThreadLocal(int __queue_size) : _M_leftover_parts(__queue_size) { }
    };

  /** @brief Balanced quicksort divide step.
    *  @param __begin Begin iterator of subsequence.
    *  @param __end End iterator of subsequence.
    *  @param __comp Comparator.
    *  @param __num_threads Number of threads that are allowed to work on
    *  this part.
    *  @pre @c (__end-__begin)>=1 */
  template<typename _RAIter, typename _Compare>
    typename std::iterator_traits<_RAIter>::difference_type
    __qsb_divide(_RAIter __begin, _RAIter __end,
		 _Compare __comp, _ThreadIndex __num_threads)
    {
      _GLIBCXX_PARALLEL_ASSERT(__num_threads > 0);

      typedef std::iterator_traits<_RAIter> _TraitsType;
      typedef typename _TraitsType::value_type _ValueType;
      typedef typename _TraitsType::difference_type _DifferenceType;

      _RAIter __pivot_pos =
	__median_of_three_iterators(__begin, __begin + (__end - __begin) / 2,
				    __end  - 1, __comp);

#if defined(_GLIBCXX_PARALLEL_ASSERTIONS)
      // Must be in between somewhere.
      _DifferenceType __n = __end - __begin;

      _GLIBCXX_PARALLEL_ASSERT((!__comp(*__pivot_pos, *__begin)
				&& !__comp(*(__begin + __n / 2),
					   *__pivot_pos))
			       || (!__comp(*__pivot_pos, *__begin)
				   && !__comp(*(__end - 1), *__pivot_pos))
			       || (!__comp(*__pivot_pos, *(__begin + __n / 2))
				   && !__comp(*__begin, *__pivot_pos))
			       || (!__comp(*__pivot_pos, *(__begin + __n / 2))
				   && !__comp(*(__end - 1), *__pivot_pos))
			       || (!__comp(*__pivot_pos, *(__end - 1))
				   && !__comp(*__begin, *__pivot_pos))
			       || (!__comp(*__pivot_pos, *(__end - 1))
				   && !__comp(*(__begin + __n / 2),
					      *__pivot_pos)));
#endif

      // Swap pivot value to end.
      if (__pivot_pos != (__end - 1))
	std::iter_swap(__pivot_pos, __end - 1);
      __pivot_pos = __end - 1;

      __gnu_parallel::__binder2nd<_Compare, _ValueType, _ValueType, bool>
	__pred(__comp, *__pivot_pos);

      // Divide, returning __end - __begin - 1 in the worst case.
      _DifferenceType __split_pos = __parallel_partition(__begin, __end - 1,
							 __pred,
							 __num_threads);

      // Swap back pivot to middle.
      std::iter_swap(__begin + __split_pos, __pivot_pos);
      __pivot_pos = __begin + __split_pos;

#if _GLIBCXX_PARALLEL_ASSERTIONS
      _RAIter __r;
      for (__r = __begin; __r != __pivot_pos; ++__r)
	_GLIBCXX_PARALLEL_ASSERT(__comp(*__r, *__pivot_pos));
      for (; __r != __end; ++__r)
	_GLIBCXX_PARALLEL_ASSERT(!__comp(*__r, *__pivot_pos));
#endif

      return __split_pos;
    }

  /** @brief Quicksort conquer step.
    *  @param __tls Array of thread-local storages.
    *  @param __begin Begin iterator of subsequence.
    *  @param __end End iterator of subsequence.
    *  @param __comp Comparator.
    *  @param __iam Number of the thread processing this function.
    *  @param __num_threads
    *          Number of threads that are allowed to work on this part. */
  template<typename _RAIter, typename _Compare>
    void
    __qsb_conquer(_QSBThreadLocal<_RAIter>** __tls,
		  _RAIter __begin, _RAIter __end,
		  _Compare __comp,
		  _ThreadIndex __iam, _ThreadIndex __num_threads,
		  bool __parent_wait)
    {
      typedef std::iterator_traits<_RAIter> _TraitsType;
      typedef typename _TraitsType::value_type _ValueType;
      typedef typename _TraitsType::difference_type _DifferenceType;

      _DifferenceType __n = __end - __begin;

      if (__num_threads <= 1 || __n <= 1)
	{
          __tls[__iam]->_M_initial.first  = __begin;
          __tls[__iam]->_M_initial.second = __end;

          __qsb_local_sort_with_helping(__tls, __comp, __iam, __parent_wait);

          return;
	}

      // Divide step.
      _DifferenceType __split_pos =
	__qsb_divide(__begin, __end, __comp, __num_threads);

#if _GLIBCXX_PARALLEL_ASSERTIONS
      _GLIBCXX_PARALLEL_ASSERT(0 <= __split_pos &&
                               __split_pos < (__end - __begin));
#endif

      _ThreadIndex
	__num_threads_leftside = std::max<_ThreadIndex>
	(1, std::min<_ThreadIndex>(__num_threads - 1, __split_pos
				   * __num_threads / __n));

#     pragma omp atomic
      *__tls[__iam]->_M_elements_leftover -= (_DifferenceType)1;

      // Conquer step.
#     pragma omp parallel num_threads(2)
      {
	bool __wait;
	if(omp_get_num_threads() < 2)
          __wait = false;
	else
          __wait = __parent_wait;

#       pragma omp sections
	{
#         pragma omp section
	  {
	    __qsb_conquer(__tls, __begin, __begin + __split_pos, __comp,
			  __iam, __num_threads_leftside, __wait);
	    __wait = __parent_wait;
	  }
	  // The pivot_pos is left in place, to ensure termination.
#         pragma omp section
	  {
	    __qsb_conquer(__tls, __begin + __split_pos + 1, __end, __comp,
			  __iam + __num_threads_leftside,
			  __num_threads - __num_threads_leftside, __wait);
	    __wait = __parent_wait;
	  }
	}
      }
    }

  /**
    *  @brief Quicksort step doing load-balanced local sort.
    *  @param __tls Array of thread-local storages.
    *  @param __comp Comparator.
    *  @param __iam Number of the thread processing this function.
    */
  template<typename _RAIter, typename _Compare>
    void
    __qsb_local_sort_with_helping(_QSBThreadLocal<_RAIter>** __tls,
				  _Compare& __comp, _ThreadIndex __iam,
				  bool __wait)
    {
      typedef std::iterator_traits<_RAIter> _TraitsType;
      typedef typename _TraitsType::value_type _ValueType;
      typedef typename _TraitsType::difference_type _DifferenceType;
      typedef std::pair<_RAIter, _RAIter> _Piece;

      _QSBThreadLocal<_RAIter>& __tl = *__tls[__iam];

      _DifferenceType
	__base_case_n = _Settings::get().sort_qsb_base_case_maximal_n;
      if (__base_case_n < 2)
	__base_case_n = 2;
      _ThreadIndex __num_threads = __tl._M_num_threads;

      // Every thread has its own random number generator.
      _RandomNumber __rng(__iam + 1);

      _Piece __current = __tl._M_initial;

      _DifferenceType __elements_done = 0;
#if _GLIBCXX_PARALLEL_ASSERTIONS
      _DifferenceType __total_elements_done = 0;
#endif

      for (;;)
	{
          // Invariant: __current must be a valid (maybe empty) range.
          _RAIter __begin = __current.first, __end = __current.second;
          _DifferenceType __n = __end - __begin;

          if (__n > __base_case_n)
            {
              // Divide.
              _RAIter __pivot_pos = __begin +  __rng(__n);

              // Swap __pivot_pos value to end.
              if (__pivot_pos != (__end - 1))
        	std::iter_swap(__pivot_pos, __end - 1);
              __pivot_pos = __end - 1;

              __gnu_parallel::__binder2nd
		<_Compare, _ValueType, _ValueType, bool>
		__pred(__comp, *__pivot_pos);

              // Divide, leave pivot unchanged in last place.
              _RAIter __split_pos1, __split_pos2;
              __split_pos1 = __gnu_sequential::partition(__begin, __end - 1,
							 __pred);

              // Left side: < __pivot_pos; __right side: >= __pivot_pos.
#if _GLIBCXX_PARALLEL_ASSERTIONS
              _GLIBCXX_PARALLEL_ASSERT(__begin <= __split_pos1
                                       && __split_pos1 < __end);
#endif
              // Swap pivot back to middle.
              if (__split_pos1 != __pivot_pos)
        	std::iter_swap(__split_pos1, __pivot_pos);
              __pivot_pos = __split_pos1;

              // In case all elements are equal, __split_pos1 == 0.
              if ((__split_pos1 + 1 - __begin) < (__n >> 7)
		  || (__end - __split_pos1) < (__n >> 7))
        	{
                  // Very unequal split, one part smaller than one 128th
                  // elements not strictly larger than the pivot.
                  __gnu_parallel::__unary_negate<__gnu_parallel::__binder1st
                    <_Compare, _ValueType, _ValueType, bool>, _ValueType>
                    __pred(__gnu_parallel::__binder1st
                	 <_Compare, _ValueType, _ValueType, bool>
			   (__comp, *__pivot_pos));

                  // Find other end of pivot-equal range.
                  __split_pos2 = __gnu_sequential::partition(__split_pos1 + 1,
							     __end, __pred);
        	}
              else
        	// Only skip the pivot.
        	__split_pos2 = __split_pos1 + 1;

              // Elements equal to pivot are done.
              __elements_done += (__split_pos2 - __split_pos1);
#if _GLIBCXX_PARALLEL_ASSERTIONS
              __total_elements_done += (__split_pos2 - __split_pos1);
#endif
              // Always push larger part onto stack.
              if (((__split_pos1 + 1) - __begin) < (__end - (__split_pos2)))
        	{
                  // Right side larger.
                  if ((__split_pos2) != __end)
                    __tl._M_leftover_parts.push_front
		      (std::make_pair(__split_pos2, __end));

                  //__current.first = __begin;    //already set anyway
                  __current.second = __split_pos1;
                  continue;
        	}
              else
        	{
                  // Left side larger.
                  if (__begin != __split_pos1)
                    __tl._M_leftover_parts.push_front(std::make_pair
						      (__begin, __split_pos1));

                  __current.first = __split_pos2;
                  //__current.second = __end;     //already set anyway
                  continue;
        	}
            }
          else
            {
              __gnu_sequential::sort(__begin, __end, __comp);
              __elements_done += __n;
#if _GLIBCXX_PARALLEL_ASSERTIONS
              __total_elements_done += __n;
#endif

              // Prefer own stack, small pieces.
              if (__tl._M_leftover_parts.pop_front(__current))
        	continue;

#             pragma omp atomic
              *__tl._M_elements_leftover -= __elements_done;

              __elements_done = 0;

#if _GLIBCXX_PARALLEL_ASSERTIONS
              double __search_start = omp_get_wtime();
#endif

              // Look for new work.
              bool __successfully_stolen = false;
              while (__wait && *__tl._M_elements_leftover > 0
                     && !__successfully_stolen
#if _GLIBCXX_PARALLEL_ASSERTIONS
                      // Possible dead-lock.
                     && (omp_get_wtime() < (__search_start + 1.0))
#endif
		     )
        	{
                  _ThreadIndex __victim;
                  __victim = __rng(__num_threads);

                  // Large pieces.
                  __successfully_stolen = (__victim != __iam)
		    && __tls[__victim]->_M_leftover_parts.pop_back(__current);
                  if (!__successfully_stolen)
                    __yield();
#if !defined(__ICC) && !defined(__ECC)
#                 pragma omp flush
#endif
        	}

#if _GLIBCXX_PARALLEL_ASSERTIONS
              if (omp_get_wtime() >= (__search_start + 1.0))
        	{
                  sleep(1);
                  _GLIBCXX_PARALLEL_ASSERT(omp_get_wtime()
                                           < (__search_start + 1.0));
        	}
#endif
              if (!__successfully_stolen)
        	{
#if _GLIBCXX_PARALLEL_ASSERTIONS
                  _GLIBCXX_PARALLEL_ASSERT(*__tl._M_elements_leftover == 0);
#endif
                  return;
        	}
            }
	}
    }

  /** @brief Top-level quicksort routine.
    *  @param __begin Begin iterator of sequence.
    *  @param __end End iterator of sequence.
    *  @param __comp Comparator.
    *  @param __num_threads Number of threads that are allowed to work on
    *  this part.
    */
  template<typename _RAIter, typename _Compare>
    void
    __parallel_sort_qsb(_RAIter __begin, _RAIter __end,
			_Compare __comp, _ThreadIndex __num_threads)
    {
      _GLIBCXX_CALL(__end - __begin)

      typedef std::iterator_traits<_RAIter> _TraitsType;
      typedef typename _TraitsType::value_type _ValueType;
      typedef typename _TraitsType::difference_type _DifferenceType;
      typedef std::pair<_RAIter, _RAIter> _Piece;

      typedef _QSBThreadLocal<_RAIter> _TLSType;

      _DifferenceType __n = __end - __begin;

      if (__n <= 1)
	return;

      // At least one element per processor.
      if (__num_threads > __n)
	__num_threads = static_cast<_ThreadIndex>(__n);

      // Initialize thread local storage
      _TLSType** __tls = new _TLSType*[__num_threads];
      _DifferenceType __queue_size = (__num_threads
				      * (_ThreadIndex)(__rd_log2(__n) + 1));
      for (_ThreadIndex __t = 0; __t < __num_threads; ++__t)
	__tls[__t] = new _QSBThreadLocal<_RAIter>(__queue_size);

      // There can never be more than ceil(__rd_log2(__n)) ranges on the
      // stack, because
      // 1. Only one processor pushes onto the stack
      // 2. The largest range has at most length __n
      // 3. Each range is larger than half of the range remaining
      volatile _DifferenceType __elements_leftover = __n;
      for (_ThreadIndex __i = 0; __i < __num_threads; ++__i)
	{
          __tls[__i]->_M_elements_leftover = &__elements_leftover;
          __tls[__i]->_M_num_threads = __num_threads;
          __tls[__i]->_M_global = std::make_pair(__begin, __end);

          // Just in case nothing is left to assign.
          __tls[__i]->_M_initial = std::make_pair(__end, __end);
	}

      // Main recursion call.
      __qsb_conquer(__tls, __begin, __begin + __n, __comp, 0,
		    __num_threads, true);

#if _GLIBCXX_PARALLEL_ASSERTIONS
      // All stack must be empty.
      _Piece __dummy;
      for (_ThreadIndex __i = 1; __i < __num_threads; ++__i)
	_GLIBCXX_PARALLEL_ASSERT(
          !__tls[__i]->_M_leftover_parts.pop_back(__dummy));
#endif

      for (_ThreadIndex __i = 0; __i < __num_threads; ++__i)
	delete __tls[__i];
      delete[] __tls;
    }
} // namespace __gnu_parallel

#endif /* _GLIBCXX_PARALLEL_BALANCED_QUICKSORT_H */
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        // -*- C++ -*-

// Copyright (C) 2007-2019 Free Software Foundation, Inc.
//
// This file is part of the GNU ISO C++ Library.  This library is free
// software; you can redistribute it and/or modify it under the terms
// of the GNU General Public License as published by the Free Software
// Foundation; either version 3, or (at your option) any later
// version.

// This library is distributed in the hope that it will be useful, but
// WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
// General Public License for more details.

// Under Section 7 of GPL version 3, you are granted additional
// permissions described in the GCC Runtime Library Exception, version
// 3.1, as published by the Free Software Foundation.

// You should have received a copy of the GNU General Public License and
// a copy of the GCC Runtime Library Exception along with this program;
// see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
// <http://www.gnu.org/licenses/>.

/** @file parallel/base.h
 *  @brief Sequential helper functions.
 *  This file is a GNU parallel extension to the Standard C++ Library.
 */

// Written by Johannes Singler.

#ifndef _GLIBCXX_PARALLEL_BASE_H
#define _GLIBCXX_PARALLEL_BASE_H 1

#include <bits/c++config.h>
#include <bits/stl_function.h>
#include <omp.h>
#include <parallel/features.h>
#include <parallel/basic_iterator.h>
#include <parallel/parallel.h>

// Parallel mode namespaces.

/**
 * @namespace std::__parallel
 * @brief GNU parallel code, replaces standard behavior with parallel behavior.
 */
namespace std _GLIBCXX_VISIBILITY(default) 
{ 
  namespace __parallel { } 
}

/**
 * @namespace __gnu_parallel
 * @brief GNU parallel code for public use.
 */
namespace __gnu_parallel
{
  // Import all the parallel versions of components in namespace std.
  using namespace std::__parallel;
}

/**
 * @namespace __gnu_sequential
 * @brief GNU sequential classes for public use.
 */
namespace __gnu_sequential 
{ 
  // Import whatever is the serial version.
#ifdef _GLIBCXX_PARALLEL
  using namespace std::_GLIBCXX_STD_A;
#else
  using namespace std;
#endif   
}


namespace __gnu_parallel
{
  // NB: Including this file cannot produce (unresolved) symbols from
  // the OpenMP runtime unless the parallel mode is actually invoked
  // and active, which imples that the OpenMP runtime is actually
  // going to be linked in.
  inline _ThreadIndex
  __get_max_threads() 
  { 
    _ThreadIndex __i = omp_get_max_threads();
    return __i > 1 ? __i : 1; 
  }


  inline bool 
  __is_parallel(const _Parallelism __p) { return __p != sequential; }


  /** @brief Calculates the rounded-down logarithm of @c __n for base 2.
   *  @param __n Argument.
   *  @return Returns 0 for any argument <1.
   */
  template<typename _Size>
    inline _Size
    __rd_log2(_Size __n)
    {
      _Size __k;
      for (__k = 0; __n > 1; __n >>= 1)
        ++__k;
      return __k;
    }

  /** @brief Encode two integers into one gnu_parallel::_CASable.
   *  @param __a First integer, to be encoded in the most-significant @c
   *  _CASable_bits/2 bits.
   *  @param __b Second integer, to be encoded in the least-significant
   *  @c _CASable_bits/2 bits.
   *  @return value encoding @c __a and @c __b.
   *  @see __decode2
   */
  inline _CASable
  __encode2(int __a, int __b)     //must all be non-negative, actually
  {
    return (((_CASable)__a) << (_CASable_bits / 2)) | (((_CASable)__b) << 0);
  }

  /** @brief Decode two integers from one gnu_parallel::_CASable.
   *  @param __x __gnu_parallel::_CASable to decode integers from.
   *  @param __a First integer, to be decoded from the most-significant
   *  @c _CASable_bits/2 bits of @c __x.
   *  @param __b Second integer, to be encoded in the least-significant
   *  @c _CASable_bits/2 bits of @c __x.
   *  @see __encode2
   */
  inline void
  __decode2(_CASable __x, int& __a, int& __b)
  {
    __a = (int)((__x >> (_CASable_bits / 2)) & _CASable_mask);
    __b = (int)((__x >>               0 ) & _CASable_mask);
  }

  //needed for parallel "numeric", even if "algorithm" not included

  /** @brief Equivalent to std::min. */
  template<typename _Tp>
    inline const _Tp&
    min(const _Tp& __a, const _Tp& __b)
    { return (__a < __b) ? __a : __b; }

  /** @brief Equivalent to std::max. */
  template<typename _Tp>
    inline const _Tp&
    max(const _Tp& __a, const _Tp& __b)
    { return (__a > __b) ? __a : __b; }

  /** @brief Constructs predicate for equality from strict weak
   *  ordering predicate
   */
  template<typename _T1, typename _T2, typename _Compare>
    class _EqualFromLess : public std::binary_function<_T1, _T2, bool>
    {
    private:
      _Compare& _M_comp;

    public:
      _EqualFromLess(_Compare& __comp) : _M_comp(__comp) { }

      bool operator()(const _T1& __a, const _T2& __b)
      { return !_M_comp(__a, __b) && !_M_comp(__b, __a); }
    };


  /** @brief Similar to std::unary_negate,
   *  but giving the argument types explicitly. */
  template<typename _Predicate, typename argument_type>
    class __unary_negate
    : public std::unary_function<argument_type, bool>
    {
    protected:
      _Predicate _M_pred;

    public:
      explicit
      __unary_negate(const _Predicate& __x) : _M_pred(__x) { }

      bool
      operator()(const argument_type& __x)
      { return !_M_pred(__x); }
    };

  /** @brief Similar to std::binder1st,
   *  but giving the argument types explicitly. */
  template<typename _Operation, typename _FirstArgumentType,
	   typename _SecondArgumentType, typename _ResultType>
    class __binder1st
    : public std::unary_function<_SecondArgumentType, _ResultType>
    {
    protected:
      _Operation _M_op;
      _FirstArgumentType _M_value;

    public:
      __binder1st(const _Operation& __x, const _FirstArgumentType& __y)
      : _M_op(__x), _M_value(__y) { }

      _ResultType
      operator()(const _SecondArgumentType& __x)
      { return _M_op(_M_value, __x); }

      // _GLIBCXX_RESOLVE_LIB_DEFECTS
      // 109.  Missing binders for non-const sequence elements
      _ResultType
      operator()(_SecondArgumentType& __x) const
      { return _M_op(_M_value, __x); }
    };

  /**
   *  @brief Similar to std::binder2nd, but giving the argument types
   *  explicitly.
   */
  template<typename _Operation, typename _FirstArgumentType,
	   typename _SecondArgumentType, typename _ResultType>
    class __binder2nd
    : public std::unary_function<_FirstArgumentType, _ResultType>
    {
    protected:
      _Operation _M_op;
      _SecondArgumentType _M_value;

    public:
      __binder2nd(const _Operation& __x, const _SecondArgumentType& __y)
      : _M_op(__x), _M_value(__y) { }

      _ResultType
      operator()(const _FirstArgumentType& __x) const
      { return _M_op(__x, _M_value); }

      // _GLIBCXX_RESOLVE_LIB_DEFECTS
      // 109.  Missing binders for non-const sequence elements
      _ResultType
      operator()(_FirstArgumentType& __x)
      { return _M_op(__x, _M_value); }
    };

  /** @brief Similar to std::equal_to, but allows two different types. */
  template<typename _T1, typename _T2>
    struct _EqualTo : std::binary_function<_T1, _T2, bool>
    {
      bool operator()(const _T1& __t1, const _T2& __t2) const
      { return __t1 == __t2; }
    };

  /** @brief Similar to std::less, but allows two different types. */
  template<typename _T1, typename _T2>
    struct _Less : std::binary_function<_T1, _T2, bool>
    {
      bool
      operator()(const _T1& __t1, const _T2& __t2) const
      { return __t1 < __t2; }

      bool
      operator()(const _T2& __t2, const _T1& __t1) const
      { return __t2 < __t1; }
    };

  // Partial specialization for one type. Same as std::less.
  template<typename _Tp>
    struct _Less<_Tp, _Tp>
    : public std::less<_Tp> { };

  /** @brief Similar to std::plus, but allows two different types. */
  template<typename _Tp1, typename _Tp2, typename _Result
	   = __typeof__(*static_cast<_Tp1*>(0)
			+ *static_cast<_Tp2*>(0))>
    struct _Plus : public std::binary_function<_Tp1, _Tp2, _Result>
    {
      _Result
      operator()(const _Tp1& __x, const _Tp2& __y) const
      { return __x + __y; }
    };

  // Partial specialization for one type. Same as std::plus.
  template<typename _Tp>
    struct _Plus<_Tp, _Tp, _Tp>
    : public std::plus<_Tp> { };

  /** @brief Similar to std::multiplies, but allows two different types. */
  template<typename _Tp1, typename _Tp2, typename _Result
	   = __typeof__(*static_cast<_Tp1*>(0)
			* *static_cast<_Tp2*>(0))>
    struct _Multiplies : public std::binary_function<_Tp1, _Tp2, _Result>
    {
      _Result
      operator()(const _Tp1& __x, const _Tp2& __y) const
      { return __x * __y; }
    };

  // Partial specialization for one type. Same as std::multiplies.
  template<typename _Tp>
    struct _Multiplies<_Tp, _Tp, _Tp>
    : public std::multiplies<_Tp> { };

  /** @brief _Iterator associated with __gnu_parallel::_PseudoSequence.
   *  If features the usual random-access iterator functionality.
   *  @param _Tp Sequence _M_value type.
   *  @param _DifferenceTp Sequence difference type.
   */
  template<typename _Tp, typename _DifferenceTp>
    class _PseudoSequenceIterator
    {
    public:
      typedef _DifferenceTp _DifferenceType;

      _PseudoSequenceIterator(const _Tp& __val, _DifferenceType __pos)
      : _M_val(__val), _M_pos(__pos) { }

      // Pre-increment operator.
      _PseudoSequenceIterator&
      operator++()
      {
	++_M_pos;
	return *this;
      }

      // Post-increment operator.
      _PseudoSequenceIterator
      operator++(int)
      { return _PseudoSequenceIterator(_M_pos++); }

      const _Tp&
      operator*() const
      { return _M_val; }

      const _Tp&
      operator[](_DifferenceType) const
      { return _M_val; }

      bool
      operator==(const _PseudoSequenceIterator& __i2)
      { return _M_pos == __i2._M_pos; }

      bool
      operator!=(const _PseudoSequenceIterator& __i2)
      { return _M_pos != __i2._M_pos; }

      _DifferenceType
      operator-(const _PseudoSequenceIterator& __i2)
      { return _M_pos - __i2._M_pos; }

    private:
      const _Tp& _M_val;
      _DifferenceType _M_pos;
    };

  /** @brief Sequence that conceptually consists of multiple copies of
      the same element.
      *  The copies are not stored explicitly, of course.
      *  @param _Tp Sequence _M_value type.
      *  @param _DifferenceTp Sequence difference type.
      */
  template<typename _Tp, typename _DifferenceTp>
    class _PseudoSequence
    {
    public:
      typedef _DifferenceTp _DifferenceType;

      // Better cast down to uint64_t, than up to _DifferenceTp.
      typedef _PseudoSequenceIterator<_Tp, uint64_t> iterator;

      /** @brief Constructor.
       *  @param __val Element of the sequence.
       *  @param __count Number of (virtual) copies.
       */
      _PseudoSequence(const _Tp& __val, _DifferenceType __count)
      : _M_val(__val), _M_count(__count)  { }

      /** @brief Begin iterator. */
      iterator
      begin() const
      { return iterator(_M_val, 0); }

      /** @brief End iterator. */
      iterator
      end() const
      { return iterator(_M_val, _M_count); }

    private:
      const _Tp& _M_val;
      _DifferenceType _M_count;
    };

  /** @brief Compute the median of three referenced elements,
      according to @c __comp.
      *  @param __a First iterator.
      *  @param __b Second iterator.
      *  @param __c Third iterator.
      *  @param __comp Comparator.
      */
  template<typename _RAIter, typename _Compare>
    _RAIter
    __median_of_three_iterators(_RAIter __a, _RAIter __b,
				_RAIter __c, _Compare __comp)
    {
      if (__comp(*__a, *__b))
	if (__comp(*__b, *__c))
	  return __b;
	else
	  if (__comp(*__a, *__c))
	    return __c;
	  else
	    return __a;
      else
	{
	  // Just swap __a and __b.
	  if (__comp(*__a, *__c))
	    return __a;
	  else
	    if (__comp(*__b, *__c))
	      return __c;
	    else
	      return __b;
	}
    }

#if _GLIBCXX_PARALLEL_ASSERTIONS && defined(__glibcxx_assert_impl)
#define _GLIBCXX_PARALLEL_ASSERT(_Condition) __glibcxx_assert_impl(_Condition)
#else
#define _GLIBCXX_PARALLEL_ASSERT(_Co